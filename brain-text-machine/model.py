import numpy as np
import math
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, Counter
import jieba
import re
import gradio as gr
import os
import zipfile
from tqdm import tqdm
import random
import platform
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
import logging
import time
from threading import Thread
from typing import List, Tuple, Optional, Deque, Dict, Any


# -------------------------- 1. åŸºç¡€é…ç½® --------------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")


# -------------------------- 2. åŸºç¡€ç»„ä»¶ç±» --------------------------
class LeakyIntegrateFire(nn.Module):
    """
    è„‰å†²ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒçš„æ¼ç§¯åˆ†å‘æ”¾æœºåˆ¶

    å‚æ•°:
        input_size (int): è¾“å…¥ç‰¹å¾ç»´åº¦
        hidden_size (int): éšè—å±‚ç»´åº¦
        beta (float): è†œç”µä½è¡°å‡ç³»æ•° (0.0-1.0)
        threshold (float): ç¥ç»å…ƒå‘æ”¾é˜ˆå€¼
        reset_value (float): å‘æ”¾åé‡ç½®ç”µä½å€¼
    """

    def __init__(self, input_size: int, hidden_size: int, beta: float = 0.9,
                 threshold: float = 1.0, reset_value: float = 0.0):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.beta = beta
        self.threshold = threshold
        self.reset_value = reset_value
        self.fc = nn.Linear(input_size, hidden_size)

    def forward(self, x: torch.Tensor, mem: Optional[torch.Tensor] = None,
                spike: Optional[torch.Tensor] = None, num_steps: int = 5) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­è¿‡ç¨‹

        å‚æ•°:
            x: è¾“å…¥å¼ é‡ (batch_size, input_size) æˆ– (batch_size, num_steps, input_size)
            mem: åˆå§‹è†œç”µä½ (batch_size, hidden_size)
            spike: åˆå§‹è„‰å†²çŠ¶æ€ (batch_size, hidden_size)
            num_steps: æ¨¡æ‹Ÿæ—¶é—´æ­¥æ•°

        è¿”å›:
            spikes: è„‰å†²åºåˆ— (batch_size, num_steps, hidden_size)
            final_mem: æœ€ç»ˆè†œç”µä½ (batch_size, hidden_size)
        """
        batch_size = x.size(0)

        # å¤„ç†è¾“å…¥å½¢çŠ¶
        if x.dim() == 3 and x.size(1) == num_steps:
            time_series = x
        else:
            time_series = x.unsqueeze(1).repeat(1, num_steps, 1)

        # åˆå§‹åŒ–çŠ¶æ€
        if mem is None:
            mem = torch.full((batch_size, self.hidden_size), self.reset_value, device=x.device)
        if spike is None:
            spike = torch.zeros((batch_size, self.hidden_size), device=x.device)

        spikes = torch.zeros((batch_size, num_steps, self.hidden_size), device=x.device)

        for step in range(num_steps):
            current = self.fc(time_series[:, step, :])
            mem = self.beta * mem + (1 - self.beta) * current
            spike = (mem >= self.threshold).float()
            mem = mem * (1 - spike) + self.reset_value * spike
            spikes[:, step, :] = spike

        return spikes, mem


class NeuroModulator:
    """
    ç¥ç»è°ƒèŠ‚æ¨¡å—ï¼Œæ¨¡æ‹Ÿå¤šå·´èƒºç³»ç»Ÿå¯¹å­¦ä¹ çš„å½±å“

    å‚æ•°:
        initial_dopamine (float): åˆå§‹å¤šå·´èƒºæ°´å¹³
        learning_rate (float): å­¦ä¹ ç‡ï¼Œæ§åˆ¶å¤šå·´èƒºæ›´æ–°é€Ÿåº¦
    """

    def __init__(self, initial_dopamine: float = 0.5, learning_rate: float = 0.01):
        self.dopamine = initial_dopamine
        self.novelty_history: Deque[float] = deque(maxlen=100)
        self.learning_rate = learning_rate

    def update_dopamine(self, novelty: float) -> float:
        """
        æ ¹æ®æ–°é¢–æ€§æ›´æ–°å¤šå·´èƒºæ°´å¹³

        å‚æ•°:
            novelty: å½“å‰åˆºæ¿€çš„æ–°é¢–æ€§è¯„åˆ† (0.0-1.0)

        è¿”å›:
            æ›´æ–°åçš„å¤šå·´èƒºæ°´å¹³
        """
        self.novelty_history.append(novelty)
        avg_novelty = np.mean(self.novelty_history) if self.novelty_history else 0.5

        # ä½¿ç”¨æ›´ç¨³å®šçš„sigmoidé£æ ¼æ›´æ–°
        difference = novelty - avg_novelty
        update = self.learning_rate * np.tanh(difference * 2)  # é™åˆ¶æ›´æ–°å¹…åº¦

        self.dopamine += update
        self.dopamine = np.clip(self.dopamine, 0.1, 0.9)
        return self.dopamine

    def get_noise_intensity(self, creativity_level: float = 1.0) -> float:
        """
        è·å–å™ªå£°å¼ºåº¦ï¼Œç”¨äºä¿ƒè¿›æ¢ç´¢

        å‚æ•°:
            creativity_level: åˆ›é€ åŠ›è°ƒèŠ‚å› å­

        è¿”å›:
            å™ªå£°å¼ºåº¦å€¼
        """
        return creativity_level * (0.5 + self.dopamine) / 1.5


# -------------------------- 3. TextDatasetï¼ˆå¼ºåˆ¶ç»Ÿä¸€æ ·æœ¬é•¿åº¦ï¼‰ --------------------------
class TextDataset(Dataset):
    """
    æ–‡æœ¬æ•°æ®é›†ç±»ï¼Œå¤„ç†å˜é•¿æ–‡æœ¬å¹¶ç»Ÿä¸€ä¸ºå›ºå®šé•¿åº¦

    å‚æ•°:
        texts: æ–‡æœ¬åˆ—è¡¨
        tokenizer: åˆ†è¯å™¨å®ä¾‹
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
    """

    def __init__(self, texts: List[str], tokenizer: 'ImprovedTokenizer', max_length: int = 50):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.input_length = max_length - 1
        self.filter_short_texts()

    def filter_short_texts(self):
        """è¿‡æ»¤è¿‡çŸ­çš„æ–‡æœ¬"""
        filtered = []
        for text in self.texts:
            tokens = self.tokenizer.tokenize(text, self.max_length)
            if len(tokens) >= 2:
                filtered.append(text)
        self.texts = filtered
        if not self.texts:
            raise ValueError(f"æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬ï¼ˆéœ€å•æ®µæ–‡æœ¬tokenæ•°â‰¥2ï¼Œå½“å‰max_length={self.max_length}ï¼‰")

    def __len__(self) -> int:
        return len(self.texts)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        text = self.texts[idx]
        tokens = self.tokenizer.tokenize(text, self.max_length)

        # ç¡®ä¿å¡«å……åé•¿åº¦æ­£ç¡®
        if len(tokens) < self.max_length:
            pad_num = self.max_length - len(tokens)
            tokens += [self.tokenizer.word2idx['<PAD>']] * pad_num
        else:
            tokens = tokens[:self.max_length]  # æˆªæ–­

        input_tokens = tokens[:self.input_length]
        target_tokens = tokens[1:self.max_length]

        # ç¡®ä¿è¾“å…¥å’Œç›®æ ‡é•¿åº¦ä¸€è‡´
        if len(input_tokens) != self.input_length:
            input_tokens = input_tokens[:self.input_length] + [self.tokenizer.word2idx['<PAD>']] * (
                        self.input_length - len(input_tokens))

        if len(target_tokens) != self.input_length:
            target_tokens = target_tokens[:self.input_length] + [self.tokenizer.word2idx['<PAD>']] * (
                        self.input_length - len(target_tokens))

        return torch.tensor(input_tokens, dtype=torch.long), torch.tensor(target_tokens, dtype=torch.long)


# -------------------------- 4. å…¶ä»–æ ¸å¿ƒç±» --------------------------
class DataProcessor:
    """
    æ•°æ®å¤„ç†ç±»ï¼Œè´Ÿè´£ZIPæ–‡ä»¶è§£å‹å’Œæ–‡æœ¬åŠ è½½

    å‚æ•°:
        data_dir: æ•°æ®å­˜å‚¨ç›®å½•
    """

    def __init__(self, data_dir: str = 'user_data'):
        self.data_dir = data_dir
        self.extracted_dir = None
        os.makedirs(self.data_dir, exist_ok=True)
        self.progress_callback = None

    def set_progress_callback(self, callback):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback

    def _update_progress(self, percent: float, message: str):
        """æ›´æ–°è¿›åº¦ä¿¡æ¯"""
        if self.progress_callback:
            try:
                self.progress_callback(percent, message)
            except Exception as e:
                logging.warning(f"è¿›åº¦æ›´æ–°å¤±è´¥: {str(e)}")
        logging.info(f"[æ•°æ®å¤„ç†] {percent:.1f}% - {message}")

    def unzip_file(self, zip_path: str) -> Tuple[str, List[str]]:
        """
        è§£å‹ZIPæ–‡ä»¶

        å‚æ•°:
            zip_path: ZIPæ–‡ä»¶è·¯å¾„

        è¿”å›:
            è§£å‹ç›®å½•å’ŒTXTæ–‡ä»¶åˆ—è¡¨
        """
        try:
            if not zipfile.is_zipfile(zip_path):
                raise ValueError(f"{zip_path} ä¸æ˜¯æœ‰æ•ˆçš„ZIPæ–‡ä»¶")
            filename_base = os.path.basename(zip_path).split('.')[0]
            self.extracted_dir = os.path.join(self.data_dir, f"extracted_{filename_base}")
            os.makedirs(self.extracted_dir, exist_ok=True)

            self._update_progress(10, "å¼€å§‹è§£å‹ZIPæ–‡ä»¶...")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                file_list = zip_ref.namelist()
                txt_files = [f for f in file_list if f.lower().endswith('.txt')]
                if not txt_files:
                    raise ValueError("ZIPæ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä»»ä½•TXTæ–‡æœ¬æ–‡ä»¶")

                total_files = len(txt_files)
                for i, file in enumerate(txt_files):
                    zip_ref.extract(file, self.extracted_dir)
                    progress = 10 + (i / total_files) * 30
                    self._update_progress(progress, f"è§£å‹å®Œæˆ: {os.path.basename(file)}")

            self._update_progress(40, f"ZIPè§£å‹å®Œæˆï¼Œå…± {len(txt_files)} ä¸ªTXTæ–‡ä»¶")
            return self.extracted_dir, txt_files
        except Exception as e:
            logging.error(f"è§£å‹å¤±è´¥: {str(e)}")
            raise

    def load_from_extracted(self, min_para_length: int = 20) -> List[str]:
        """
        ä»è§£å‹ç›®å½•åŠ è½½æ–‡æœ¬

        å‚æ•°:
            min_para_length: æœ€å°æ®µè½é•¿åº¦

        è¿”å›:
            æœ‰æ•ˆæ–‡æœ¬åˆ—è¡¨
        """
        if not self.extracted_dir or not os.path.exists(self.extracted_dir):
            raise FileNotFoundError(f"è§£å‹ç›®å½•ä¸å­˜åœ¨: {self.extracted_dir}")

        self._update_progress(45, "å¼€å§‹è¯»å–TXTæ–‡æœ¬...")
        texts = []
        txt_files = []
        for root, _, files in os.walk(self.extracted_dir):
            for f in files:
                if f.lower().endswith('.txt'):
                    txt_files.append(os.path.join(root, f))

        if not txt_files:
            raise ValueError("è§£å‹ç›®å½•ä¸­æœªæ‰¾åˆ°TXTæ–‡ä»¶")

        total_files = len(txt_files)
        for i, file_path in enumerate(txt_files):
            try:
                # å°è¯•UTF-8ç¼–ç 
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                except UnicodeDecodeError:
                    # å›é€€åˆ°GBKç¼–ç 
                    try:
                        with open(file_path, 'r', encoding='gbk') as f:
                            content = f.read().strip()
                    except Exception as e:
                        logging.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {str(e)}")
                        continue

                if not content:
                    logging.warning(f"è·³è¿‡ç©ºæ–‡ä»¶: {file_path}")
                    continue

                cleaned_text = self.clean_text(content)
                paragraphs = re.split(r'[\nã€‚ï¼Ÿï¼;ï¼›]+', cleaned_text)
                valid_paragraphs = [p.strip() for p in paragraphs if len(p.strip()) >= min_para_length]
                texts.extend(valid_paragraphs)

                progress = 45 + (i / total_files) * 25
                self._update_progress(progress, f"è¯»å–å®Œæˆ: {os.path.basename(file_path)}")
            except Exception as e:
                logging.error(f"å¤„ç†æ–‡ä»¶ {file_path} å‡ºé”™: {str(e)}")
                continue

        if not texts:
            raise ValueError(f"æœªåŠ è½½åˆ°æœ‰æ•ˆæ–‡æœ¬ï¼ˆå•æ®µæ–‡æœ¬éœ€â‰¥{min_para_length}å­—ç¬¦ï¼‰")

        self._update_progress(70, f"æ–‡æœ¬åŠ è½½å®Œæˆï¼Œå…± {len(texts)} æ®µæœ‰æ•ˆæ–‡æœ¬")
        return texts

    def clean_text(self, text: str) -> str:
        """æ–‡æœ¬æ¸…æ´—"""
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚,ã€ï¼›;ï¼š:â€˜â€™"\'()ï¼ˆï¼‰\[\]{}!?ï¼Ÿï¼.Â·\s]', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()


class ImprovedTokenizer:
    """
    æ”¹è¿›çš„åˆ†è¯å™¨ç±»ï¼Œæ”¯æŒè‡ªå®šä¹‰è¯å…¸å’Œå¹¶è¡Œåˆ†è¯

    å‚æ•°:
        max_vocab_size: æœ€å¤§è¯æ±‡è¡¨å¤§å°
        custom_dict: è‡ªå®šä¹‰è¯å…¸è·¯å¾„
        min_word_freq: æœ€å°è¯é¢‘é˜ˆå€¼
    """

    def __init__(self, max_vocab_size: int = 10000, custom_dict: Optional[str] = None, min_word_freq: int = 1):
        self.word2idx = {'<PAD>': 0, '<UNK>': 1}
        self.idx2word = {0: '<PAD>', 1: '<UNK>'}
        self.max_vocab_size = max_vocab_size
        self.vocab_size = 2
        self.word_counter = Counter()
        self.min_word_freq = min_word_freq
        self.progress_callback = None

        if custom_dict and os.path.exists(custom_dict):
            try:
                jieba.load_userdict(custom_dict)
                logging.info(f"å·²åŠ è½½è‡ªå®šä¹‰è¯å…¸: {custom_dict}")
            except Exception as e:
                logging.warning(f"åŠ è½½è‡ªå®šä¹‰è¯å…¸å¤±è´¥: {str(e)}")

    def set_progress_callback(self, callback):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback

    def _update_progress(self, percent: float, message: str):
        """æ›´æ–°è¿›åº¦ä¿¡æ¯"""
        if self.progress_callback:
            try:
                self.progress_callback(percent, message)
            except Exception as e:
                logging.warning(f"åˆ†è¯è¿›åº¦æ›´æ–°å¤±è´¥: {str(e)}")
        logging.info(f"[åˆ†è¯å™¨] {percent:.1f}% - {message}")

    def fit(self, texts: List[str]):
        """æ„å»ºè¯æ±‡è¡¨"""
        self._update_progress(75, "å¼€å§‹åˆ†è¯ä¸è¯æ±‡ç»Ÿè®¡...")
        use_parallel = False
        total_chars = sum(len(text) for text in texts)
        if platform.system() != "Windows" and total_chars >= 10000:
            try:
                jieba.enable_parallel()
                use_parallel = True
                logging.info(f"å¯ç”¨å¹¶è¡Œåˆ†è¯ï¼ˆæ–‡æœ¬æ€»å­—ç¬¦æ•°: {total_chars}ï¼‰")
            except Exception as e:
                logging.warning(f"å¯ç”¨å¹¶è¡Œå¤±è´¥ï¼Œé™çº§ä¸ºå•çº¿ç¨‹: {str(e)}")
        else:
            logging.info(f"ä½¿ç”¨å•çº¿ç¨‹åˆ†è¯ï¼ˆç³»ç»Ÿ: {platform.system()}, æ–‡æœ¬æ€»å­—ç¬¦æ•°: {total_chars}ï¼‰")

        all_words = []
        total_texts = len(texts)
        for i, text in enumerate(texts):
            if not text or not isinstance(text, str):
                continue
            try:
                words = jieba.cut(text, cut_all=False)
                valid_words = [w.strip() for w in words if w.strip() and len(w.strip()) >= 1]
                all_words.extend(valid_words)
            except Exception as e:
                logging.warning(f"æ–‡æœ¬åˆ†è¯å¤±è´¥: {str(e)}")
                continue

            if i % max(1, total_texts // 20) == 0:
                progress = 75 + (i / total_texts) * 20
                self._update_progress(progress, f"å¤„ç†æ–‡æœ¬ {i + 1}/{total_texts}")

        if use_parallel:
            try:
                jieba.disable_parallel()
            except:
                pass

        if not all_words:
            raise ValueError("æœªç»Ÿè®¡åˆ°ä»»ä½•è¯æ±‡ï¼ˆè¯·æ£€æŸ¥æ–‡æœ¬æœ‰æ•ˆæ€§ï¼‰")
        self.word_counter = Counter(all_words)
        sorted_words = self.word_counter.most_common(self.max_vocab_size - 2)

        self.vocab_size = 2
        for word, count in sorted_words:
            if count >= self.min_word_freq and self.vocab_size < self.max_vocab_size:
                self.word2idx[word] = self.vocab_size
                self.idx2word[self.vocab_size] = word
                self.vocab_size += 1

        if self.vocab_size <= 2:
            raise ValueError(f"è¯æ±‡è¡¨æ„å»ºå¤±è´¥ï¼ˆä»… {self.vocab_size} ä¸ªé»˜è®¤tokenï¼‰ï¼Œè¯·å¢åŠ æ–‡æœ¬å¤šæ ·æ€§")

        self._update_progress(100, f"è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œè¯æ±‡é‡: {self.vocab_size}")

    def tokenize(self, text: str, max_length: Optional[int] = None) -> List[int]:
        """æ–‡æœ¬åˆ†è¯"""
        if not text or not isinstance(text, str):
            return [self.word2idx['<PAD>']] * (max_length if max_length else 1)
        words = jieba.cut(text, cut_all=False)
        tokens = []
        for word in words:
            token_idx = self.word2idx.get(word, self.word2idx['<UNK>'])
            tokens.append(token_idx)
            if max_length and len(tokens) >= max_length:
                break
        if max_length and len(tokens) < max_length:
            tokens += [self.word2idx['<PAD>']] * (max_length - len(tokens))
        return tokens

    def detokenize(self, tokens: List[int]) -> str:
        """tokenåºåˆ—è½¬æ–‡æœ¬"""
        if not tokens:
            return ""
        words = []
        for token in tokens:
            if token == self.word2idx['<PAD>']:
                continue
            word = self.idx2word.get(token, '<UNK>')
            words.append(word)
        return ''.join(words)

    def batch_tokenize(self, texts: List[str], max_length: Optional[int] = None, padding: bool = True) -> List[
        List[int]]:
        """æ‰¹é‡åˆ†è¯"""
        tokenized = [self.tokenize(text, max_length) for text in texts]
        if max_length and padding:
            tokenized = [
                t[:max_length] if len(t) >= max_length
                else t + [self.word2idx['<PAD>']] * (max_length - len(t))
                for t in tokenized
            ]
        return tokenized


class BrainTextModel(nn.Module):
    """
    ç±»è„‘æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆLSTMå’Œè„‰å†²ç¥ç»ç½‘ç»œ

    å‚æ•°:
        vocab_size: è¯æ±‡è¡¨å¤§å°
        embedding_dim: è¯åµŒå…¥ç»´åº¦
        hidden_dim: LSTMéšè—å±‚ç»´åº¦
        concept_dim: æ¦‚å¿µç©ºé—´ç»´åº¦
        memory_size: è®°å¿†ç¼“å†²åŒºå¤§å°
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
    """

    def __init__(self, vocab_size: int, embedding_dim: int = 64, hidden_dim: int = 128,
                 concept_dim: int = 64, memory_size: int = 50, max_length: int = 50):
        super().__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.concept_dim = concept_dim
        self.max_length = max_length
        self.input_length = max_length - 1

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=1, dropout=0.1)
        self.snn = LeakyIntegrateFire(hidden_dim, hidden_dim)
        self.concept_projection = nn.Linear(hidden_dim, concept_dim)
        self.modulator = NeuroModulator()
        self.memory = deque(maxlen=memory_size)
        self.output_layer = nn.Linear(concept_dim, vocab_size)
        self.adaptation_layer = nn.Linear(concept_dim, embedding_dim)
        self.generator_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=1, dropout=0.1)

    def forward(self, x: torch.Tensor, num_steps: int = 5, return_sequence: bool = False) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            x: è¾“å…¥tokenåºåˆ— (batch_size, seq_len)
            num_steps: SNNæ¨¡æ‹Ÿæ­¥æ•°
            return_sequence: æ˜¯å¦è¿”å›å®Œæ•´åºåˆ—

        è¿”å›:
            æ¦‚å¿µç©ºé—´è¡¨ç¤º
        """
        batch_size, seq_len = x.size()
        x_emb = self.embedding(x)
        lstm_out, _ = self.lstm(x_emb)

        if return_sequence:
            batch_seq_out = lstm_out.reshape(-1, self.hidden_dim)
            spikes, _ = self.snn(batch_seq_out, num_steps=num_steps)
            spike_rate = spikes.mean(dim=1)
            spike_rates = spike_rate.reshape(batch_size, seq_len, self.hidden_dim)
            concepts = self.concept_projection(spike_rates)
            return concepts
        else:
            last_out = lstm_out[:, -1, :]
            spikes, mem = self.snn(last_out, num_steps=num_steps)
            spike_rate = spikes.mean(dim=1)
            concept = self.concept_projection(spike_rate)
            return concept

    def generate_concept(self, input_text: str, tokenizer: ImprovedTokenizer,
                         creativity_level: float = 1.0, num_steps: int = 5) -> torch.Tensor:
        """
        ç”Ÿæˆæ¦‚å¿µå‘é‡

        å‚æ•°:
            input_text: è¾“å…¥æ–‡æœ¬
            tokenizer: åˆ†è¯å™¨å®ä¾‹
            creativity_level: åˆ›é€ åŠ›æ°´å¹³
            num_steps: SNNæ¨¡æ‹Ÿæ­¥æ•°

        è¿”å›:
            æ¦‚å¿µç©ºé—´å‘é‡
        """
        self.eval()
        tokens = tokenizer.tokenize(input_text, self.max_length)
        input_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)

        with torch.no_grad():
            seq_concepts = self.forward(input_tensor, num_steps=num_steps, return_sequence=True).squeeze(0)
            weights = torch.exp(torch.linspace(-1, 0, seq_concepts.size(0))).to(seq_concepts.device)
            weights = weights / weights.sum()
            base_concept = (seq_concepts * weights.unsqueeze(1)).sum(dim=0)

        noise_intensity = self.modulator.get_noise_intensity(creativity_level)
        noise = torch.randn_like(base_concept) * noise_intensity
        memory_influence = torch.zeros_like(base_concept)

        if self.memory:
            num_memories = min(3, len(self.memory))
            selected_memories = random.sample(self.memory, num_memories)
            similarities = [
                torch.nn.functional.cosine_similarity(base_concept.unsqueeze(0), mem.unsqueeze(0)).item()
                for mem in selected_memories
            ]
            memory_weights = torch.softmax(torch.tensor(similarities) + 0.1, dim=0).to(base_concept.device)
            for i, mem in enumerate(selected_memories):
                memory_influence += memory_weights[i] * mem

        levy_step = self.levy_flight(base_concept.size(), creativity_level)
        new_concept = base_concept + noise * 0.3 + memory_influence * 0.2 + levy_step * 0.1
        self.memory.append(new_concept.detach())
        return new_concept

    def levy_flight(self, size: Tuple[int], intensity: float = 1.0) -> torch.Tensor:
        """
        åˆ—ç»´é£è¡Œéšæœºæ¸¸èµ°

        å‚æ•°:
            size: è¾“å‡ºå¼ é‡å½¢çŠ¶
            intensity: å¼ºåº¦å› å­

        è¿”å›:
            éšæœºæ­¥é•¿å¼ é‡
        """
        alpha = 1.5
        sigma = (math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /
                 (math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)
        u = torch.randn(size) * sigma
        v = torch.randn(size)
        step = u / torch.abs(v) ** (1 / alpha)
        return intensity * 0.1 * step.to(device)

    def concept_to_text(self, concept: torch.Tensor, tokenizer: ImprovedTokenizer,
                        max_length: int = 30, temperature: float = 0.7, top_p: float = 0.9) -> str:
        """
        æ¦‚å¿µå‘é‡è½¬æ–‡æœ¬ - æ”¹è¿›ç‰ˆï¼ŒåŠ å…¥é‡å¤æƒ©ç½šå’ŒTop-pé‡‡æ ·

        å‚æ•°:
            concept: æ¦‚å¿µå‘é‡
            tokenizer: åˆ†è¯å™¨å®ä¾‹
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦ (0.5-1.5)
            top_p: Top-pé‡‡æ ·é˜ˆå€¼ (0.7-0.95)

        è¿”å›:
            ç”Ÿæˆæ–‡æœ¬
        """
        self.eval()
        start_token = tokenizer.word2idx.get('<PAD>', 0)
        current_token = torch.tensor([[start_token]], dtype=torch.long).to(device)
        generated_tokens = []
        adapted_concept = self.adaptation_layer(concept).unsqueeze(0)
        lstm_hidden = (torch.zeros(1, 1, self.hidden_dim).to(device),
                       torch.zeros(1, 1, self.hidden_dim).to(device))

        unk_idx = tokenizer.word2idx['<UNK>']
        pad_idx = tokenizer.word2idx['<PAD>']
        punctuation = {tokenizer.word2idx.get(c, -1) for c in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', ';', 'ï¼›']}
        consecutive_unk = 0

        # åº”ç”¨é‡å¤æƒ©ç½šå‡½æ•°
        def apply_repetition_penalty(logits, generated_tokens, penalty=1.2):
            """
            åº”ç”¨é‡å¤æƒ©ç½šï¼Œé™ä½å·²ç”Ÿæˆtokençš„æ¦‚ç‡
            """
            # åªè€ƒè™‘æœ€è¿‘ç”Ÿæˆçš„tokenä»¥é¿å…è¿‡åº¦æƒ©ç½š
            recent_tokens = generated_tokens[-10:] if len(generated_tokens) > 10 else generated_tokens
            for token in set(recent_tokens):
                logits[0, token] /= penalty
            return logits

        for _ in range(max_length):
            emb = self.embedding(current_token)
            combined = emb + adapted_concept.unsqueeze(1) * 0.5
            lstm_out, lstm_hidden = self.generator_lstm(combined, lstm_hidden)
            logits = self.output_layer(self.concept_projection(lstm_out.squeeze(1)))

            # åº”ç”¨é‡å¤æƒ©ç½š
            logits = apply_repetition_penalty(logits, generated_tokens, penalty=1.2)

            # é™ä½æœªçŸ¥è¯å’Œå¡«å……è¯çš„æ¦‚ç‡
            logits[:, unk_idx] *= 0.01
            logits[:, pad_idx] = -float('inf')

            # åº”ç”¨æ¸©åº¦è°ƒæ•´
            logits = logits / temperature

            # ä½¿ç”¨Top-pé‡‡æ ·
            probs = torch.softmax(logits, dim=-1)
            sorted_probs, sorted_indices = torch.sort(probs, descending=True)
            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

            # ç§»é™¤ç´¯ç§¯æ¦‚ç‡å¤§äºtop_pçš„token
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0

            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            probs[indices_to_remove] = 0

            # é‡æ–°å½’ä¸€åŒ–æ¦‚ç‡
            probs = probs / probs.sum(dim=-1, keepdim=True)

            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            next_token = torch.multinomial(probs, num_samples=1)

            next_token_val = next_token.item()
            generated_tokens.append(next_token_val)

            if next_token_val == unk_idx:
                consecutive_unk += 1
                if consecutive_unk >= 2:
                    break
            else:
                consecutive_unk = 0

            current_token = next_token

            # æå‰ç»ˆæ­¢æ¡ä»¶ï¼šå¦‚æœç”Ÿæˆäº†æ ‡ç‚¹ç¬¦å·ï¼Œæœ‰50%çš„æ¦‚ç‡æå‰ç»“æŸ
            if len(generated_tokens) > max_length // 2 and next_token_val in punctuation:
                if random.random() < 0.5:
                    break

        # æ¸…ç†ç”Ÿæˆç»“æœï¼šç§»é™¤æœ«å°¾çš„æœªçŸ¥è¯ã€å¡«å……è¯å’Œæ ‡ç‚¹ç¬¦å·
        while generated_tokens and (generated_tokens[-1] in [unk_idx, pad_idx] or generated_tokens[-1] in punctuation):
            generated_tokens.pop()

        return tokenizer.detokenize(generated_tokens)

    def calculate_novelty(self, concept: torch.Tensor) -> float:
        """
        è®¡ç®—æ¦‚å¿µæ–°é¢–æ€§

        å‚æ•°:
            concept: æ¦‚å¿µå‘é‡

        è¿”å›:
            æ–°é¢–æ€§è¯„åˆ† (0.0-1.0)
        """
        if not self.memory:
            return 0.5
        similarities = [
            torch.nn.functional.cosine_similarity(concept.unsqueeze(0), mem.unsqueeze(0)).item()
            for mem in self.memory
        ]
        avg_similarity = np.mean(similarities)
        return 1 - avg_similarity

    def train_model(self, dataloader: DataLoader, val_dataloader: Optional[DataLoader] = None,
                    epochs: int = 5, lr: float = 0.001, patience: int = 2):
        """
        æ¨¡å‹è®­ç»ƒ

        å‚æ•°:
            dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            lr: å­¦ä¹ ç‡
            patience: æ—©åœè€å¿ƒå€¼
        """
        device = next(self.parameters()).device
        criterion = nn.CrossEntropyLoss(ignore_index=0)
        optimizer = optim.Adam(self.parameters(), lr=lr)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, factor=0.5)
        best_val_loss = float('inf')
        patience_counter = 0

        # ä¿®å¤ï¼šæ·»åŠ è®­ç»ƒæ¨¡å¼æ ‡è®°
        self.optimizer = optimizer

        for epoch in range(epochs):
            # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
            self.train()  # æ·»åŠ è¿™è¡Œå…³é”®ä»£ç 

            total_loss = 0
            accumulation_steps = 4
            optimizer.zero_grad()

            for i, (inputs, targets) in enumerate(dataloader):
                assert inputs.size(1) == self.input_length, f"è¾“å…¥åºåˆ—é•¿åº¦é”™è¯¯: {inputs.size(1)}â‰ {self.input_length}"
                assert targets.size(1) == self.input_length, f"ç›®æ ‡åºåˆ—é•¿åº¦é”™è¯¯: {targets.size(1)}â‰ {self.input_length}"

                inputs = inputs.to(device)
                targets = targets.to(device)
                batch_size, seq_len = inputs.size()

                concepts = self.forward(inputs, return_sequence=True)
                adapted = self.adaptation_layer(concepts)
                lstm_out, _ = self.generator_lstm(adapted)
                logits = self.output_layer(self.concept_projection(lstm_out))
                loss = criterion(logits.transpose(1, 2), targets) / accumulation_steps

                total_loss += loss.item() * accumulation_steps
                loss.backward()

                if (i + 1) % accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
                    optimizer.step()
                    optimizer.zero_grad()

            avg_loss = total_loss / len(dataloader)
            val_loss = 0

            if val_dataloader:
                self.eval()
                with torch.no_grad():
                    for inputs, targets in val_dataloader:
                        assert inputs.size(
                            1) == self.input_length, f"éªŒè¯è¾“å…¥é•¿åº¦é”™è¯¯: {inputs.size(1)}â‰ {self.input_length}"
                        assert targets.size(
                            1) == self.input_length, f"éªŒè¯ç›®æ ‡é•¿åº¦é”™è¯¯: {targets.size(1)}â‰ {self.input_length}"

                        inputs = inputs.to(device)
                        targets = targets.to(device)
                        concepts = self.forward(inputs, return_sequence=True)
                        adapted = self.adaptation_layer(concepts)
                        lstm_out, _ = self.generator_lstm(adapted)
                        logits = self.output_layer(self.concept_projection(lstm_out))
                        loss = criterion(logits.transpose(1, 2), targets)
                        val_loss += loss.item()
                val_loss /= len(val_dataloader)
                logging.info(f"Epoch {epoch + 1} å¹³å‡æŸå¤±: {avg_loss:.4f}, éªŒè¯æŸå¤±: {val_loss:.4f}")

                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        logging.info(f"éªŒè¯æŸå¤±è¿ç»­ {patience} æ¬¡æœªæ”¹å–„ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                        break
            else:
                logging.info(f"Epoch {epoch + 1} å¹³å‡æŸå¤±: {avg_loss:.4f}")

            scheduler.step(avg_loss if not val_dataloader else val_loss)
            logging.info(f"å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")


# -------------------------- 5. Gradioç•Œé¢ï¼ˆä¿®å¤tqdmè¿›åº¦æ¡æ­¥æ•°è·å–ï¼‰ --------------------------
def build_interface():
    logger.info("å¼€å§‹æ„å»ºç•Œé¢...")
    processor = DataProcessor()
    tokenizer = None
    model = None

    def upload_zip_file(file, progress=gr.Progress()):
        nonlocal processor
        if file is None:
            return "è¯·ä¸Šä¼ åŒ…å«TXTæ–‡ä»¶çš„ZIPåŒ…ï¼ˆæ”¯æŒä»»æ„å¤§å°ï¼‰"
        try:
            processor.set_progress_callback(lambda p, d: progress(p, desc=d))
            extract_dir, txt_files = processor.unzip_file(file.name)
            return f"ZIPå¤„ç†æˆåŠŸï¼\nè§£å‹ç›®å½•: {extract_dir}\nåŒ…å« {len(txt_files)} ä¸ªTXTæ–‡ä»¶"
        except Exception as e:
            return f"ZIPå¤„ç†å¤±è´¥: {str(e)}"

    def prepare_data(custom_dict_file, progress=gr.Progress()):
        nonlocal processor, tokenizer
        if not processor.extracted_dir:
            return "è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†ZIPæ–‡ä»¶"
        try:
            texts = processor.load_from_extracted(min_para_length=20)
            custom_dict_path = custom_dict_file.name if custom_dict_file else None
            tokenizer = ImprovedTokenizer(
                max_vocab_size=10000,
                custom_dict=custom_dict_path,
                min_word_freq=1
            )
            tokenizer.set_progress_callback(lambda p, d: progress(p, desc=d))
            tokenizer.fit(texts)
            return f"æ•°æ®å‡†å¤‡å®Œæˆï¼\næœ‰æ•ˆæ–‡æœ¬æ®µæ•°: {len(texts)}\nè¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}"
        except Exception as e:
            logger.error(f"æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}")
            return f"æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}"

    def train_model_interface(epochs, embedding_dim, hidden_dim, max_length, progress=gr.Progress()):
        nonlocal tokenizer, model, processor
        if not tokenizer:
            return "è¯·å…ˆä¸Šä¼ ZIPå¹¶å‡†å¤‡æ•°æ®"
        try:
            progress(0, desc="å‡†å¤‡è®­ç»ƒæ•°æ®...")
            texts = processor.load_from_extracted()
            dataset = TextDataset(texts, tokenizer, max_length=int(max_length))
            if len(dataset) < 5:
                return f"è®­ç»ƒæ•°æ®ä¸è¶³ï¼ˆä»… {len(dataset)} ä¸ªæ ·æœ¬ï¼‰ï¼Œè‡³å°‘éœ€è¦5ä¸ªæ ·æœ¬"

            test_size = 0.2 if len(dataset) >= 10 else 0.1 if len(dataset) >= 5 else 0
            val_loader = None
            if test_size > 0:
                train_dataset, val_dataset = train_test_split(dataset, test_size=test_size, random_state=42)
                val_loader = DataLoader(val_dataset, batch_size=min(8, len(val_dataset)), shuffle=False)
            else:
                train_dataset = dataset

            batch_size = min(16, len(train_dataset)) if len(train_dataset) >= 16 else min(4, len(train_dataset))
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

            progress(0.3, desc="åˆå§‹åŒ–æ¨¡å‹...")
            model = BrainTextModel(
                vocab_size=tokenizer.vocab_size,
                embedding_dim=embedding_dim,
                hidden_dim=hidden_dim,
                max_length=int(max_length)
            ).to(device)

            progress(0.5, desc="å¼€å§‹è®­ç»ƒæ¨¡å‹...")
            original_tqdm_update = tqdm.update

            # -------------------------- ä¿®å¤ï¼šé€šè¿‡è¿›åº¦æ¡å®ä¾‹è·å–å·²è¿­ä»£æ­¥æ•° --------------------------
            def tqdm_update_wrapper(*args, **kwargs):
                # è°ƒç”¨åŸå§‹updateæ–¹æ³•ï¼ˆä¿ç•™æ‰€æœ‰å‚æ•°ï¼‰
                original_tqdm_update(*args, **kwargs)
                # é€šè¿‡è¿›åº¦æ¡å®ä¾‹çš„nå±æ€§è·å–å·²è¿­ä»£æ­¥æ•°ï¼ˆtqdmå†…éƒ¨ç»´æŠ¤ï¼‰
                current_step = progress_bar.n
                total_steps = len(train_loader)
                # è®¡ç®—è¿›åº¦ï¼ˆç¡®ä¿ä¸è¶…è¿‡0.99ï¼‰
                current_progress = 0.5 + (current_step / total_steps) * 0.5
                progress(min(current_progress, 0.99), desc=f"è®­ç»ƒä¸­: {current_step}/{total_steps} æ­¥")

            # åˆ›å»ºtqdmè¿›åº¦æ¡å®ä¾‹å¹¶ä¿å­˜å¼•ç”¨
            progress_bar = tqdm(train_loader, desc="è®­ç»ƒè¿›åº¦")
            tqdm.update = tqdm_update_wrapper  # æ›¿æ¢tqdmçš„updateæ–¹æ³•

            model.train_model(train_loader, val_loader, epochs=epochs, lr=0.001)

            # æ¢å¤åŸå§‹updateæ–¹æ³•
            tqdm.update = original_tqdm_update

            progress(1.0, desc="è®­ç»ƒå®Œæˆï¼")
            val_count = len(val_dataset) if 'val_dataset' in locals() else 0
            return f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼\nè®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}\néªŒè¯æ ·æœ¬æ•°: {val_count}\nåºåˆ—é•¿åº¦: {model.input_length}\næœ€ç»ˆå­¦ä¹ ç‡: {model.optimizer.param_groups[0]['lr']:.6f}"
        except Exception as e:
            logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
            return f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}"

    def generate_text_interface(input_text, creativity_level, gen_length, top_p_value):
        nonlocal tokenizer, model
        if not model or not tokenizer:
            return "è¯·å…ˆå®Œæˆæ•°æ®å‡†å¤‡å’Œæ¨¡å‹è®­ç»ƒ"
        if not input_text or input_text.strip() == "":
            input_text = "è¯·è¾“å…¥æœ‰æ„ä¹‰çš„æç¤ºæ–‡æœ¬"
        try:
            concept = model.generate_concept(
                input_text,
                tokenizer,
                creativity_level=creativity_level
            )
            novelty = model.calculate_novelty(concept)
            dopamine_level = model.modulator.update_dopamine(novelty)

            # æ ¹æ®åˆ›æ–°ç¨‹åº¦è°ƒæ•´é‡‡æ ·å‚æ•°
            # é«˜åˆ›æ–°åº¦ï¼šæ›´é«˜çš„æ¸©åº¦ï¼Œæ›´ä½çš„Top-på€¼ï¼ˆå¢åŠ éšæœºæ€§ï¼‰
            # ä½åˆ›æ–°åº¦ï¼šæ›´ä½çš„æ¸©åº¦ï¼Œæ›´é«˜çš„Top-på€¼ï¼ˆå‡å°‘éšæœºæ€§ï¼‰
            temp = 0.7 + (creativity_level - 1.0) * 0.3
            top_p = 0.95 - (creativity_level - 1.0) * 0.2  # èŒƒå›´0.7-0.95

            generated_text = model.concept_to_text(
                concept,
                tokenizer,
                max_length=int(gen_length),
                temperature=temp,
                top_p=top_p_value  # ä½¿ç”¨ä¼ å…¥çš„top_på€¼
            )

            generated_text = generated_text.strip()
            if not generated_text:
                return "ç”Ÿæˆæ–‡æœ¬ä¸ºç©ºï¼Œè¯·è°ƒæ•´åˆ›æ–°åº¦æˆ–æç¤ºæ–‡æœ¬"
            if generated_text[-1] not in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']:
                generated_text += 'ã€‚'
            info = f"\n\n=== ç”Ÿæˆä¿¡æ¯ ===\næ–°é¢–æ€§è¯„åˆ†: {novelty:.2f}\nå¤šå·´èƒºæ°´å¹³: {dopamine_level:.2f}\né‡‡æ ·æ¸©åº¦: {temp:.2f}\nTop-på€¼: {top_p:.2f}\næç¤ºæ–‡æœ¬: {input_text[:50]}..."
            return generated_text + info
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}"

    with gr.Blocks(title="é€šç”¨ç±»è„‘æ–‡æœ¬ç”Ÿæˆç³»ç»Ÿ") as demo:
        gr.Markdown("# ğŸ§  é€šç”¨ç±»è„‘æ–‡æœ¬ç”Ÿæˆç³»ç»Ÿ")
        gr.Markdown("ä¿®å¤DataLoaderæ— nå±æ€§é—®é¢˜ï¼Œæ”¯æŒä»»æ„å¤§å°æ–‡æœ¬")

        with gr.Tab("1. ä¸Šä¼ ZIPæ–‡ä»¶"):
            zip_file = gr.File(label="ä¸Šä¼ åŒ…å«TXTçš„ZIPåŒ…ï¼ˆæ”¯æŒä»»æ„å¤§å°ï¼‰", file_types=[".zip"])
            upload_btn = gr.Button("å¤„ç†ZIPæ–‡ä»¶", variant="primary")
            upload_output = gr.Textbox(label="å¤„ç†ç»“æœ", lines=3)

        with gr.Tab("2. å‡†å¤‡è®­ç»ƒæ•°æ®"):
            custom_dict = gr.File(label="å¯é€‰ï¼šè‡ªå®šä¹‰è¯å…¸ï¼ˆTXTæ ¼å¼ï¼‰", file_types=[".txt"])
            prepare_btn = gr.Button("å‡†å¤‡æ•°æ®ï¼ˆåˆ†è¯+è¯æ±‡è¡¨ï¼‰", variant="primary")
            prepare_output = gr.Textbox(label="å‡†å¤‡ç»“æœ", lines=3)

        with gr.Tab("3. è®­ç»ƒæ¨¡å‹"):
            with gr.Row():
                epochs = gr.Slider(minimum=1, maximum=20, value=5, step=1, label="è®­ç»ƒè½®æ¬¡")
                embedding_dim = gr.Slider(minimum=32, maximum=256, value=64, step=32, label="åµŒå…¥ç»´åº¦")
                hidden_dim = gr.Slider(minimum=64, maximum=512, value=128, step=64, label="éšè—å±‚ç»´åº¦")
                max_length = gr.Slider(minimum=10, maximum=100, value=50, step=5, label="åºåˆ—æœ€å¤§é•¿åº¦ï¼ˆè¾“å…¥+ç›®æ ‡ï¼‰")
            train_btn = gr.Button("å¼€å§‹è®­ç»ƒæ¨¡å‹", variant="primary")
            train_output = gr.Textbox(label="è®­ç»ƒç»“æœ", lines=3)

        with gr.Tab("4. ç”Ÿæˆæ–‡æœ¬"):
            input_text = gr.Textbox(
                label="è¾“å…¥æç¤ºæ–‡æœ¬ï¼ˆä»»æ„ä¸»é¢˜ï¼‰",
                value="äººå·¥æ™ºèƒ½çš„å‘å±•å¯¹äººç±»è®¤çŸ¥æœ‰ä½•å½±å“ï¼Ÿ",
                lines=2,
                placeholder="è¯·è¾“å…¥ä»»æ„ä¸»é¢˜çš„æç¤ºæ–‡æœ¬..."
            )
            with gr.Row():
                creativity = gr.Slider(minimum=0.5, maximum=2.0, value=1.0, step=0.1, label="åˆ›æ–°ç¨‹åº¦")
                gen_length = gr.Slider(minimum=30, maximum=300, value=100, step=10, label="ç”Ÿæˆæ–‡æœ¬é•¿åº¦")
                top_p = gr.Slider(minimum=0.7, maximum=0.95, value=0.9, step=0.05, label="Top-pé‡‡æ ·å€¼")
            generate_btn = gr.Button("ç”Ÿæˆæ–‡æœ¬", variant="primary")
            output_text = gr.Textbox(label="ç”Ÿæˆæ–‡æœ¬", lines=8)

        # æ›´æ–°äº‹ä»¶ç»‘å®š
        generate_btn.click(generate_text_interface, inputs=[input_text, creativity, gen_length, top_p],
                           outputs=output_text)
        # ç»‘å®šZIPä¸Šä¼ æŒ‰é’®
        upload_btn.click(upload_zip_file, inputs=[zip_file], outputs=upload_output)

        # ç»‘å®šæ•°æ®å‡†å¤‡æŒ‰é’®
        prepare_btn.click(prepare_data, inputs=[custom_dict], outputs=prepare_output)

        # ç»‘å®šè®­ç»ƒæ¨¡å‹æŒ‰é’®
        train_btn.click(
            train_model_interface,
            inputs=[epochs, embedding_dim, hidden_dim, max_length],
            outputs=train_output
        )

    logger.info("ç•Œé¢æ„å»ºå®Œæˆ")
    return demo


# -------------------------- ä¸»ç¨‹åº --------------------------
if __name__ == "__main__":
    logger.info("è¿›å…¥ä¸»ç¨‹åºæ‰§è¡Œ...")
    demo = build_interface()
    logger.info("å¯åŠ¨Gradioç•Œé¢...")
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        debug=False,
        quiet=False,
        share=False
    )